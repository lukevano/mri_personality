{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb90771",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbb3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_CNN = '/Users/DrV/PyProj/MRI_personality/sub-0001/nibabel_CNN'\n",
    "path_to_excel = '/Users/DrV/code/lukevano/tables_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c8ebcd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b79a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/3.8.6/envs/mri_personality/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe8db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c7848a",
   "metadata": {},
   "source": [
    "# Log Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e88985",
   "metadata": {},
   "source": [
    "## Description of loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d74722",
   "metadata": {},
   "source": [
    "Three datasets are all merged in the following way:\n",
    "\n",
    "Top 208 observations are ds002785, from 209 to 434 are ds002790, and from 435 to 1362 are ds003097\n",
    "\n",
    "Columns have been concatenated so that the first columns are the participant details, then the targets, followed by the lt then rt cortical parcellations and lastly the volume segmentation of the subcortical structures.\n",
    "\n",
    "Columns religious and raven have been deleted as not recorded in all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5e2935",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/DrV/code/lukevano/tables_2/master_combined.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hg/ml24knyn6djby4wjfxt7vnvr0000gn/T/ipykernel_98371/408265249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/DrV/code/lukevano/tables_2/master_combined.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/mri_personality/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/mri_personality/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/mri_personality/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/mri_personality/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     ) as handle:\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/mri_personality/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/DrV/code/lukevano/tables_2/master_combined.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel('/Users/DrV/code/lukevano/tables_2/master_combined.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the volume and thickness features and view correlations\n",
    "vol_thick_features = df.iloc[:,11:]\n",
    "corr = vol_thick_features.corr()\n",
    "corr_df = corr.unstack().reset_index() # Unstack correlation matrix \n",
    "corr_df.columns = ['feature_1','feature_2', 'correlation'] # rename columns\n",
    "corr_df.sort_values(by=\"correlation\",ascending=False, inplace=True) # sort by correlation\n",
    "corr_df = corr_df[corr_df['feature_1'] != corr_df['feature_2']] # Remove self correlation\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the columns that are identical\n",
    "corr_df[corr_df.correlation>0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the columns to remove and consider removing\n",
    "# Remove idenical columns and hypointensity columns with virtually no values\n",
    "to_remove = ['eTIV.1', 'EstimatedTotalIntraCranialVol', 'BrainSegVolNotVent.2',\n",
    "    'BrainSegVolNotVent.1', 'BrainSegVolNotVentSurf', 'SupraTentorialVolNotVentVox',\n",
    "    'lhCerebralWhiteMatterVol', 'rhCerebralWhiteMatterVol', 'BrainSegVolNotVent.2', \n",
    "    'BrainSegVol', 'SupraTentorialVol', 'SupraTentorialVolNotVent',\n",
    "    'BrainSegVol-to-eTIV', 'MaskVol', 'rhCortexVol', 'lhCortexVol', 'Left-WM-hypointensities',\n",
    "    'Right-WM-hypointensities', 'non-WM-hypointensities', 'Left-non-WM-hypointensities',\n",
    "    'Right-non-WM-hypointensities']\n",
    "consider_removal = ['5th-Ventricle', 'SurfaceHoles', 'lhSurfaceHoles', 'rhSurfaceHoles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c566c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_thick_features_less = vol_thick_features.drop(columns=to_remove)\n",
    "corr_less = vol_thick_features_less.corr()\n",
    "corr_less_df = corr_less.unstack().reset_index() # Unstack correlation matrix \n",
    "corr_less_df.columns = ['feature_1','feature_2', 'correlation'] # rename columns\n",
    "corr_less_df.sort_values(by=\"correlation\",ascending=False, inplace=True) # sort by correlation\n",
    "corr_less_df = corr_less_df[corr_less_df['feature_1'] != corr_less_df['feature_2']] # Remove self correlation\n",
    "corr_less_df[corr_less_df.correlation>0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e19fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.corr()['NEO_N'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.corr()['NEO_E'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.corr()['NEO_O'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dade57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.corr()['NEO_A'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97dd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.corr()['NEO_C'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4a485",
   "metadata": {},
   "source": [
    "## Preprocessing master_dup_removed v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75187c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_neo_nan = pd.read_excel('/Users/DrV/code/lukevano/tables_2/master_dup_removed v1.xlsx')\n",
    "df_neo_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in main feat/targets\n",
    "df_neo_nan.isnull().sum().sort_values(ascending=False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f30666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nan from NEO_C\n",
    "df = df_neo_nan[df_neo_nan.NEO_C.notnull()].reset_index(drop=True) # reset or have empty indexes\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77780de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing target: 0 = lower, 1 = higher\n",
    "y_disc = df.iloc[:,6:11]\n",
    "y = []\n",
    "for i in range(5):\n",
    "    y.append(pd.cut(x= y_disc.iloc[:,i], \n",
    "                    bins= [y_disc.iloc[:,i].min()-1, y_disc.iloc[:,i].mean(), y_disc.iloc[:,i].max()+1], \n",
    "                    labels= [0, 1]))\n",
    "y = pd.DataFrame(y[0:5]).T\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature creation\n",
    "X = df.iloc[:,11:]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that no null in features and targets\n",
    "print(X.isnull().sum().sum())\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504b32d",
   "metadata": {},
   "source": [
    "## Building models and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57371702",
   "metadata": {},
   "source": [
    "Use statsmodels over sklearn as you can determine more about the features. When doing logisitic regression you are sacrificing quality for explainability: you know what is happening behind the model.\n",
    "\n",
    "The two approaches for building models are either make a model with each feature individually and then start adding the features together or put all the features together and start to remove those that are too highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f194aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant as a feature- needed unless you want to go through origin on Logit\n",
    "X_int = sm.add_constant(X)\n",
    "X_int.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee68b10",
   "metadata": {},
   "source": [
    "Build the models and work out what features to keep for each. If you standardise then you lose some information but it allows you to compare coeffiecients: if the feature is on a larger scale smaller coefficients make have a larger impact on the model. The null hypothesis is that the coef is 0 so if p>0.05 then can't prove not 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the output for a non-standardised model\n",
    "# Will take longer to converge if not scaled= 41 iterations. Get the same accuracy:\n",
    "# It will always converge on the global minima as Logit is a convex problem\n",
    "results_NEO_N = sm.Logit(y['NEO_N'], X_int).fit(maxiter=100)\n",
    "results_NEO_N.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62face5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant as a feature to the scaled X and make DF\n",
    "X_scaled_int = sm.add_constant(X_scaled)\n",
    "X_scaled_int = pd.DataFrame(X_scaled_int, columns=X_int.columns)\n",
    "X_scaled_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the output for a standardised model\n",
    "results_NEO_N_scaled = sm.Logit(y['NEO_N'], X_scaled_int).fit()\n",
    "results_NEO_N_scaled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aef49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the thickness col with p<0.25\n",
    "NEO_N_col_thick_25 = ['lh_bankssts_thickness', 'lh_lateralorbitofrontal_thickness', \n",
    "                  'lh_parahippocampal_thickness', 'lh_parsorbitalis_thickness',\n",
    "                  'lh_postcentral_thickness', 'lh_precentral_thickness',\n",
    "                  'lh_superiorfrontal_thickness', 'lh_superiorparietal_thickness',\n",
    "                  'lh_supramarginal_thickness', 'rh_caudalmiddlefrontal_thickness',\n",
    "                  'rh_entorhinal_thickness', 'rh_inferiorparietal_thickness',\n",
    "                  'rh_lateraloccipital_thickness', 'rh_lateralorbitofrontal_thickness',\n",
    "                  'rh_middletemporal_thickness', 'rh_parahippocampal_thickness',\n",
    "                  'rh_precentral_thickness', 'rh_superiorfrontal_thickness',\n",
    "                  'rh_superiorparietal_thickness', 'rh_superiortemporal_thickness',\n",
    "                  'rh_MeanThickness_thickness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_NEO_N_25 = X_scaled_int[NEO_N_col_thick_25]\n",
    "X_NEO_N_25.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf238d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the output for a non-standardised model\n",
    "results_NEO_N_25 = sm.Logit(y['NEO_N'], X_NEO_N_25).fit()\n",
    "results_NEO_N_25.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred and y pred proba for the models\n",
    "y_pred_proba_NEO_N = results_NEO_N.predict(X_int) # remember to add int to features\n",
    "y_pred_proba_NEO_N_scaled = results_NEO_N_scaled.predict(X_scaled_int)\n",
    "y_pred_proba_NEO_N_25 = results_NEO_N_25.predict(X_NEO_N_25)\n",
    "\n",
    "y_pred_NEO_N = results_NEO_N.predict(X_int)>0.5\n",
    "y_pred_NEO_N_scaled = results_NEO_N_scaled.predict(X_scaled_int)>0.5\n",
    "y_pred_NEO_N_25 = results_NEO_N_25.predict(X_NEO_N_25)>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14dccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(classification_report(y['NEO_N'], y_pred_NEO_N))\n",
    "print(classification_report(y['NEO_N'], y_pred_NEO_N_scaled))\n",
    "print(classification_report(y['NEO_N'], y_pred_NEO_N_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9309b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract associated metrics and thresholds then work out AUC\n",
    "fpr, tpr, thresholds = roc_curve(y['NEO_N'], y_pred_proba_NEO_N)\n",
    "scores = pd.DataFrame({'threshold':thresholds,\n",
    "                       'fpr':fpr,\n",
    "                       'tpr':tpr})\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846dd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores['fpr'],scores['tpr']);\n",
    "plt.ylabel('tpr');\n",
    "plt.xlabel('fpr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbe9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = roc_auc_score(y['NEO_N'], y_pred_NEO_N)\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd670aa",
   "metadata": {},
   "source": [
    "## Building seperate models M and F models for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sex col- drop nan and convert to single char\n",
    "sex_df = df\n",
    "print(sex_df.sex.unique())\n",
    "sex_df.sex.isnull().sum()\n",
    "sex_df = sex_df[sex_df.sex.notnull()]\n",
    "sex_df.sex.replace('female', 'F', inplace=True)\n",
    "sex_df.sex.replace('male', 'M', inplace=True)\n",
    "print(sex_df.sex.unique())\n",
    "sex_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3952acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make male and female df\n",
    "f_df = sex_df[sex_df.sex=='F']\n",
    "m_df = sex_df[sex_df.sex=='M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature creation\n",
    "X_f = f_df.iloc[:,11:].reset_index(drop=True)\n",
    "X_m = m_df.iloc[:,11:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target creation: 0 = lower, 1 = higher\n",
    "y_f_disc = f_df.iloc[:,6:11].reset_index(drop=True)\n",
    "y_f = []\n",
    "for i in range(5):\n",
    "    y_f.append(pd.cut(x= y_f_disc.iloc[:,i], \n",
    "                    bins= [y_f_disc.iloc[:,i].min()-1, y_f_disc.iloc[:,i].mean(), y_f_disc.iloc[:,i].max()+1], \n",
    "                    labels= [0, 1]))\n",
    "y_f = pd.DataFrame(y_f[0:5]).T.reset_index(drop=True)\n",
    "\n",
    "y_m_disc = m_df.iloc[:,6:11].reset_index(drop=True)\n",
    "y_m = []\n",
    "for i in range(5):\n",
    "    y_m.append(pd.cut(x= y_m_disc.iloc[:,i], \n",
    "                    bins= [y_m_disc.iloc[:,i].min()-1, y_m_disc.iloc[:,i].mean(), y_m_disc.iloc[:,i].max()+1], \n",
    "                    labels= [0, 1]))\n",
    "y_m = pd.DataFrame(y_m[0:5]).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7614a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "scaler_f = StandardScaler()\n",
    "X_f_scaled = scaler_f.fit_transform(X_f)\n",
    "\n",
    "scaler_m = StandardScaler()\n",
    "X_m_scaled = scaler_m.fit_transform(X_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant as a feature to the scaled X\n",
    "X_f_scaled_int = sm.add_constant(X_f_scaled)\n",
    "X_f_scaled_int = pd.DataFrame(X_f_scaled_int, columns=pd.Index(['const']).append(X_f.columns))\n",
    "\n",
    "X_m_scaled_int = sm.add_constant(X_m_scaled)\n",
    "X_m_scaled_int = pd.DataFrame(X_m_scaled_int, columns=pd.Index(['const']).append(X_f.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cc51c",
   "metadata": {},
   "source": [
    "In a log reg the coef below show you how the model is made. The const coef means that the value of the const in the feat will be multiplied by the coef- this is why we add a 1 in the const column. The coef in the other columns is the formula for the logit equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c060086",
   "metadata": {},
   "source": [
    "NEO_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de64707",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_NEO_N = sm.Logit(y_f['NEO_N'], X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_NEO_N.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfe7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_NEO_N = sm.Logit(y_m['NEO_N'], X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_NEO_N.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred and y pred proba for the models\n",
    "y_pred_f_proba_NEO_N = results_f_NEO_N.predict(X_f_scaled_int)\n",
    "y_pred_m_proba_NEO_N = results_m_NEO_N.predict(X_m_scaled_int)\n",
    "\n",
    "y_pred_f_NEO_N = results_f_NEO_N.predict(X_f_scaled_int)>0.5\n",
    "y_pred_m_NEO_N = results_m_NEO_N.predict(X_m_scaled_int)>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a687c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(classification_report(y_f['NEO_N'], y_pred_f_NEO_N))\n",
    "print(classification_report(y_m['NEO_N'], y_pred_m_NEO_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d99ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_f_NEO_N_score = roc_auc_score(y_f['NEO_N'], y_pred_f_NEO_N)\n",
    "auc_m_NEO_N_score = roc_auc_score(y_m['NEO_N'], y_pred_m_NEO_N)\n",
    "print(auc_f_NEO_N_score)\n",
    "auc_m_NEO_N_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b102e0",
   "metadata": {},
   "source": [
    "NEO_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfba1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_NEO_E = sm.Logit(y_f['NEO_E'], X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_NEO_E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_NEO_E = sm.Logit(y_m['NEO_E'], X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_NEO_E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred and y pred proba for the models\n",
    "y_pred_f_proba_NEO_E = results_f_NEO_E.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_proba_NEO_E = results_m_NEO_E.predict(X_m_scaled_int)\n",
    "y_pred_f_NEO_E = results_f_NEO_E.predict(X_f_scaled_int)>0.5\n",
    "y_pred_m_NEO_E = results_m_NEO_E.predict(X_m_scaled_int)>0.5\n",
    "# Print classification reports\n",
    "print(classification_report(y_f['NEO_E'], y_pred_f_NEO_E))\n",
    "print(classification_report(y_m['NEO_E'], y_pred_m_NEO_E))\n",
    "auc_f_NEO_E_score = roc_auc_score(y_f['NEO_E'], y_pred_f_NEO_E)\n",
    "auc_m_NEO_E_score = roc_auc_score(y_m['NEO_E'], y_pred_m_NEO_E)\n",
    "print(auc_f_NEO_E_score)\n",
    "auc_m_NEO_E_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac8353",
   "metadata": {},
   "source": [
    "NEO_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_NEO_O = sm.Logit(y_f['NEO_O'], X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_NEO_O.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17affa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_NEO_O = sm.Logit(y_m['NEO_O'], X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_NEO_O.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred and y pred proba for the models\n",
    "y_pred_f_proba_NEO_O = results_f_NEO_O.predict(X_f_scaled_int)\n",
    "y_pred_m_proba_NEO_O = results_m_NEO_O.predict(X_m_scaled_int)\n",
    "y_pred_f_NEO_O = results_f_NEO_O.predict(X_f_scaled_int)>0.5\n",
    "y_pred_m_NEO_O = results_m_NEO_O.predict(X_m_scaled_int)>0.5\n",
    "# Print classification reports\n",
    "print(classification_report(y_f['NEO_O'], y_pred_f_NEO_O))\n",
    "print(classification_report(y_m['NEO_O'], y_pred_m_NEO_O))\n",
    "auc_f_NEO_O_score = roc_auc_score(y_f['NEO_O'], y_pred_f_NEO_O)\n",
    "auc_m_NEO_O_score = roc_auc_score(y_m['NEO_O'], y_pred_m_NEO_O)\n",
    "print(auc_f_NEO_O_score)\n",
    "auc_m_NEO_O_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb29c5",
   "metadata": {},
   "source": [
    "NEO_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7027ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_NEO_A = sm.Logit(y_f['NEO_A'], X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_NEO_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_NEO_A = sm.Logit(y_m['NEO_A'], X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_NEO_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred and y pred proba for the models\n",
    "y_pred_f_proba_NEO_A = results_f_NEO_A.predict(X_f_scaled_int)\n",
    "y_pred_m_proba_NEO_A = results_m_NEO_A.predict(X_m_scaled_int)\n",
    "y_pred_f_NEO_A = results_f_NEO_A.predict(X_f_scaled_int)>0.5\n",
    "y_pred_m_NEO_A = results_m_NEO_A.predict(X_m_scaled_int)>0.5\n",
    "# Print classification reports\n",
    "print(classification_report(y_f['NEO_A'], y_pred_f_NEO_A))\n",
    "print(classification_report(y_m['NEO_A'], y_pred_m_NEO_A))\n",
    "auc_f_NEO_A_score = roc_auc_score(y_f['NEO_A'], y_pred_f_NEO_A)\n",
    "auc_m_NEO_A_score = roc_auc_score(y_m['NEO_A'], y_pred_m_NEO_A)\n",
    "print(auc_f_NEO_A_score)\n",
    "auc_m_NEO_A_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa621a",
   "metadata": {},
   "source": [
    "NEO_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_NEO_C = sm.Logit(y_f['NEO_C'], X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_NEO_C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_NEO_C = sm.Logit(y_m['NEO_C'], X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_NEO_C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred and y pred proba for the models\n",
    "y_pred_f_proba_NEO_C = results_f_NEO_C.predict(X_f_scaled_int)\n",
    "y_pred_m_proba_NEO_C = results_m_NEO_C.predict(X_m_scaled_int)\n",
    "y_pred_f_NEO_C = results_f_NEO_C.predict(X_f_scaled_int)>0.5\n",
    "y_pred_m_NEO_C = results_m_NEO_C.predict(X_m_scaled_int)>0.5\n",
    "# Print classification reports\n",
    "print(classification_report(y_f['NEO_C'], y_pred_f_NEO_C))\n",
    "print(classification_report(y_m['NEO_C'], y_pred_m_NEO_C))\n",
    "auc_f_NEO_C_score = roc_auc_score(y_f['NEO_C'], y_pred_f_NEO_C)\n",
    "auc_m_NEO_C_score = roc_auc_score(y_m['NEO_C'], y_pred_m_NEO_C)\n",
    "print(auc_f_NEO_C_score)\n",
    "auc_m_NEO_C_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3214e9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a2cc4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Show the most important features for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c815c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_N_f_best_feats = pd.DataFrame([results_f_NEO_N.params, results_f_NEO_N.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_N_f_best_feats[NEO_N_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76be63",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_N_m_best_feats = pd.DataFrame([results_m_NEO_N.params, results_m_NEO_N.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_N_m_best_feats[NEO_N_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de0eb5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_E_f_best_feats = pd.DataFrame([results_f_NEO_E.params, results_f_NEO_E.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_E_f_best_feats[NEO_E_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da4b34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_E_m_best_feats = pd.DataFrame([results_m_NEO_E.params, results_m_NEO_E.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_E_m_best_feats[NEO_E_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59badd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_O_f_best_feats = pd.DataFrame([results_f_NEO_O.params, results_f_NEO_O.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_O_f_best_feats[NEO_O_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae74a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_O_m_best_feats = pd.DataFrame([results_m_NEO_O.params, results_m_NEO_O.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_O_m_best_feats[NEO_O_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e3282",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_A_f_best_feats = pd.DataFrame([results_f_NEO_A.params, results_f_NEO_A.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_A_f_best_feats[NEO_A_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772e989",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_A_m_best_feats = pd.DataFrame([results_m_NEO_A.params, results_m_NEO_A.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_A_m_best_feats[NEO_A_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231b447",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_C_f_best_feats = pd.DataFrame([results_f_NEO_C.params, results_f_NEO_C.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_C_f_best_feats[NEO_C_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7a546",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NEO_C_m_best_feats = pd.DataFrame([results_m_NEO_C.params, results_m_NEO_C.pvalues], index=['coef', 'p-value']).T\n",
    "NEO_C_m_best_feats[NEO_C_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c77a35",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to do feature permutation need to make an sklearn model as this function will not work with statsmodel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1ed63",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NEO_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21eb6ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Use this to get an idea how sklearn models are created\n",
    "# # need to remove const from coef and put this in int- then use X without const\n",
    "# sk_lr = LogisticRegression()\n",
    "# sk_lr.fit(X_f_scaled_int, y_f['NEO_N'])\n",
    "# sk_lr.coef_\n",
    "# sk_lr.intercept_\n",
    "# sk_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928663e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_NEO_N.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_NEO_N.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_NEO_N_score = permutation_importance(sk_lr, X_f_scaled, y_f['NEO_N'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_NEO_N = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_NEO_N_score.importances_mean)).T) # Unstack results\n",
    "importance_f_NEO_N.columns=['feature','score decrease']\n",
    "importance_f_NEO_N.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_NEO_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9faf03",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_f_NEO_N.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015c10c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_m_NEO_N.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_m_NEO_N.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_m_NEO_N_score = permutation_importance(sk_lr, X_m_scaled, y_m['NEO_N'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_m_NEO_N = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_m_NEO_N_score.importances_mean)).T) # Unstack results\n",
    "importance_m_NEO_N.columns=['feature','score decrease']\n",
    "importance_m_NEO_N.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_m_NEO_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733dfa1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_m_NEO_N.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8032f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NEO_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557d065",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_NEO_E.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_NEO_E.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_NEO_E_score = permutation_importance(sk_lr, X_f_scaled, y_f['NEO_E'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_NEO_E = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_NEO_E_score.importances_mean)).T) # Unstack results\n",
    "importance_f_NEO_E.columns=['feature','score decrease']\n",
    "importance_f_NEO_E.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_NEO_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7113992",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_f_NEO_E.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f05345",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_m_NEO_E.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_m_NEO_E.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_m_NEO_E_score = permutation_importance(sk_lr, X_m_scaled, y_m['NEO_E'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_m_NEO_E = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_m_NEO_E_score.importances_mean)).T) # Unstack results\n",
    "importance_m_NEO_E.columns=['feature','score decrease']\n",
    "importance_m_NEO_E.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_m_NEO_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a381d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_m_NEO_E.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b75e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_NEO_O.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_NEO_O.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_NEO_O_score = permutation_importance(sk_lr, X_f_scaled, y_f['NEO_O'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_NEO_O = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_NEO_O_score.importances_mean)).T) # Unstack results\n",
    "importance_f_NEO_O.columns=['feature','score decrease']\n",
    "importance_f_NEO_O.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_NEO_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c942a09",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_f_NEO_O.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc978db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_m_NEO_O.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_m_NEO_O.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_m_NEO_O_score = permutation_importance(sk_lr, X_m_scaled, y_m['NEO_O'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_m_NEO_O = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_m_NEO_O_score.importances_mean)).T) # Unstack results\n",
    "importance_m_NEO_O.columns=['feature','score decrease']\n",
    "importance_m_NEO_O.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_m_NEO_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c49d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_m_NEO_O.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69161bf7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_NEO_A.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_NEO_A.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_NEO_A_score = permutation_importance(sk_lr, X_f_scaled, y_f['NEO_A'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_NEO_A = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_NEO_A_score.importances_mean)).T) # Unstack results\n",
    "importance_f_NEO_A.columns=['feature','score decrease']\n",
    "importance_f_NEO_A.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_NEO_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe9668",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_f_NEO_A.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb643c9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_m_NEO_A.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_m_NEO_A.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_m_NEO_A_score = permutation_importance(sk_lr, X_m_scaled, y_m['NEO_A'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_m_NEO_A = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_m_NEO_A_score.importances_mean)).T) # Unstack results\n",
    "importance_m_NEO_A.columns=['feature','score decrease']\n",
    "importance_m_NEO_A.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_m_NEO_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baf876",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_m_NEO_A.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63bb36",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_NEO_C.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_NEO_C.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_NEO_C_score = permutation_importance(sk_lr, X_f_scaled, y_f['NEO_C'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_NEO_C = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_NEO_C_score.importances_mean)).T) # Unstack results\n",
    "importance_f_NEO_C.columns=['feature','score decrease']\n",
    "importance_f_NEO_C.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_NEO_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe85fbb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_f_NEO_C.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3c7f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_m_NEO_C.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_m_NEO_C.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_m_NEO_C_score = permutation_importance(sk_lr, X_m_scaled, y_m['NEO_C'], scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_m_NEO_C = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_m_NEO_C_score.importances_mean)).T) # Unstack results\n",
    "importance_m_NEO_C.columns=['feature','score decrease']\n",
    "importance_m_NEO_C.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_m_NEO_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ab246",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The model is just using size of the largest areas!\n",
    "importance_m_NEO_C.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ab239",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942a0de",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Female model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f8def",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_lin_f_NEO_N = sm.OLS(y_f_disc['NEO_N'], X_f_scaled_int).fit(maxiter=100)\n",
    "results_lin_f_NEO_N.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1417ff1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make y pred\n",
    "y_pred_lin_f_NEO_N = results_lin_f_NEO_N.predict(X_f_scaled_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257e27d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Female regression metrics\n",
    "mse_f = mean_squared_error(y_f_disc['NEO_N'], y_pred_lin_f_NEO_N)\n",
    "rmse_f = math.sqrt(mse_f)\n",
    "mae_f = mean_absolute_error(y_f_disc['NEO_N'], y_pred_lin_f_NEO_N)\n",
    "rsquared_f = r2_score(y_f_disc['NEO_N'], y_pred_lin_f_NEO_N)\n",
    "max_error_f = max_error(y_f_disc['NEO_N'], y_pred_lin_f_NEO_N)\n",
    "print('MSE =', round(mse_f, 2))\n",
    "print('RMSE =', round(rmse_f, 2))\n",
    "print('MAE =', round(mae_f, 2))\n",
    "print('R2 =', round(rsquared_f, 2))\n",
    "print('Max Error =', round(max_error_f, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f2c98",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44c224",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # equivalent but with SGD solver\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# svc_bis = SGDClassifier(loss='hinge', penalty='l2', alpha=1/10)\n",
    "# # Plot your instantiated classifier \n",
    "# from utils.plots import plot_decision_regions\n",
    "# plot_decision_regions(X, y, classifier=svm_10) # svm_10 is the model\n",
    "# # Do train/test split\n",
    "# X_f_train, y_f_train, X_f_test, y_f_test = train_test_split(X_f,y_f,test_size=0.3)\n",
    "# X_m_train, y_m_train, X_m_test, y_m_test = train_test_split(X_m,y_m,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15847ce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SVM Classification\n",
    "svc_1 = SVC(kernel='rbf', C=1) # Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3040f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(svc_1, X_f_scaled_int, y_f['NEO_N'], cv=5, scoring=['accuracy', \n",
    "                                                                                'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13d04d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0e0fd",
   "metadata": {},
   "source": [
    "# Logistic Regression with created targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec07ac",
   "metadata": {},
   "source": [
    "Use KMeans and Gaussian clustering to generate new targets for the model- This will take into account the overlap of personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sex = sex_df.iloc[:, 6:11]\n",
    "clusters = KMeans(n_clusters=5, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "y_scaled = pd.DataFrame(scaler.fit_transform(y_sex), columns=y_disc.columns)\n",
    "clusters.fit_transform(y_scaled)\n",
    "\n",
    "cluster_names = ['Leader', 'Task Orientated', 'Maverick', 'Anxious', 'Workaholic']\n",
    "\n",
    "centers = pd.DataFrame(scaler.inverse_transform(clusters.cluster_centers_),\n",
    "                       columns=y_disc.columns, index=cluster_names)\n",
    "print('KNN centers')\n",
    "print('')\n",
    "print(centers) # The centers may be different from what others use- They often change position too\n",
    "print('')\n",
    "print('KNN normalised')\n",
    "centers2 = pd.DataFrame(clusters.cluster_centers_,columns=y_disc.columns,index=cluster_names)\n",
    "centers2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40019bbb",
   "metadata": {},
   "source": [
    "Leader (Calm extrovert open agreeable organised)\\\n",
    "Task Orientated (Calm slightly extrovert close-minded organised)\\\n",
    "Maverick (Slightly neurotic slightly extrovert open-ish disagreeable disorganised)\\\n",
    "Anxious (Anxious introvert open-ish agreeable disorganised)\\\n",
    "Workaholic (Introvert closeminded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luke's personality\n",
    "luke = pd.DataFrame({'NEO_N':-1.28, 'NEO_E':1.3, 'NEO_O':-0.62, 'NEO_A':0.65, 'NEO_C':0.62}, index=['Luke'])\n",
    "luke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487cf454",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.predict(luke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maria's personality\n",
    "maria = pd.DataFrame({'NEO_N':-1.04, 'NEO_E':0.63, 'NEO_O':0.65, 'NEO_A':1.86, 'NEO_C':1.54}, index=['Maria'])\n",
    "maria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af795973",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.predict(maria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ca189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the KNN labels to the master df\n",
    "target_df = sex_df\n",
    "knn_label = clusters.labels_\n",
    "target_df['knn_label'] = knn_label\n",
    "target_df.knn_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c140c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian clustering\n",
    "gaussian_clusters = gm = GaussianMixture(n_components=5, random_state=0)\n",
    "gaussian_clusters.fit(y_scaled)\n",
    "gaussian_centers = pd.DataFrame(scaler.inverse_transform(gaussian_clusters.means_),columns=y_disc.columns)\n",
    "gaussian_label = gaussian_clusters.predict(y_scaled)\n",
    "target_df['gaussian_label'] = gaussian_label\n",
    "target_df.gaussian_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gaussian centers')\n",
    "print('')\n",
    "print(gaussian_centers) # The centers may be different from what others use- They often change position too\n",
    "print('')\n",
    "print('Gaussian normalised')\n",
    "centers2 = pd.DataFrame(gaussian_clusters.means_,columns=y_disc.columns)\n",
    "centers2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec223099",
   "metadata": {},
   "source": [
    "0: Leadership (Calm extrovert slightly agreeable organised)\\\n",
    "1: Isolatedness (Introvert close-minded disagreeable slightly-disorganised)\\\n",
    "2: Efficiency (Agreeable organised)\\\n",
    "3: Anxiety (Anxious introvert open-ish agreeable disorganised)\\\n",
    "4: Criminality (Slightly neurotic slightly extrovert open-ish disagreeable disorganised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed018a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8eecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make male and female df\n",
    "f_target_df = target_df[sex_df.sex=='F'].reset_index(drop=True)\n",
    "m_target_df = target_df[sex_df.sex=='M'].reset_index(drop=True)\n",
    "y_f_knn = f_target_df.knn_label\n",
    "y_m_knn = m_target_df.knn_label\n",
    "y_f_gaussian = f_target_df.gaussian_label\n",
    "y_m_gaussian = m_target_df.gaussian_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_knn.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_m_knn.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd96a9",
   "metadata": {},
   "source": [
    "### KNN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_knn_0 = sm.Logit(y_f_knn==0, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_knn_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_knn_1 = sm.Logit(y_f_knn==1, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_knn_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_knn_2 = sm.Logit(y_f_knn==2, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_knn_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_knn_3 = sm.Logit(y_f_knn==3, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_knn_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909da48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_knn_4 = sm.Logit(y_f_knn==4, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_knn_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c51d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_knn_0 = sm.Logit(y_m_knn==0, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_knn_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_knn_1 = sm.Logit(y_m_knn==1, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_knn_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_knn_2 = sm.Logit(y_m_knn==2, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_knn_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30908ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_knn_3 = sm.Logit(y_m_knn==3, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_knn_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_knn_4 = sm.Logit(y_m_knn==4, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_knn_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3397b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred proba for the models\n",
    "\n",
    "y_pred_f_knn_0_proba = results_f_knn_0.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_knn_0_proba = results_m_knn_0.predict(X_m_scaled_int)\n",
    "y_pred_f_knn_1_proba = results_f_knn_1.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_knn_1_proba = results_m_knn_1.predict(X_m_scaled_int)\n",
    "y_pred_f_knn_2_proba = results_f_knn_2.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_knn_2_proba = results_m_knn_2.predict(X_m_scaled_int)\n",
    "y_pred_f_knn_3_proba = results_f_knn_3.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_knn_3_proba = results_m_knn_3.predict(X_m_scaled_int)\n",
    "y_pred_f_knn_4_proba = results_f_knn_4.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_knn_4_proba = results_m_knn_4.predict(X_m_scaled_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_knn_proba = pd.DataFrame({'Leader': y_pred_f_knn_0_proba,\n",
    "                            'Task orientatied': y_pred_f_knn_1_proba,\n",
    "                            'Maverick': y_pred_f_knn_2_proba,\n",
    "                            'Anxious': y_pred_f_knn_3_proba,\n",
    "                            'Workaholic': y_pred_f_knn_4_proba})\n",
    "f_knn_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_knn_proba = pd.DataFrame({0: y_pred_m_knn_0_proba,\n",
    "                            1: y_pred_m_knn_1_proba,\n",
    "                            2: y_pred_m_knn_2_proba,\n",
    "                            3: y_pred_m_knn_3_proba,\n",
    "                            4: y_pred_m_knn_4_proba})\n",
    "m_knn_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922db375",
   "metadata": {},
   "source": [
    "KNN OvM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163de19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict={'Leader':0.57,'Task oriented':0.53,'Maverick':0.54,'Anxious':0.51,'Workaholic':.54}\n",
    "m_dict={'Leader':0.64,'Task oriented':0.53,'Maverick':0.54,'Anxious':0.51,'Workaholic':.54}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76673912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_f_knn, np.argmax(f_knn_proba.values, axis = 1)))\n",
    "print(classification_report(y_m_knn, np.argmax(m_knn_proba.values, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c2f60",
   "metadata": {},
   "source": [
    "## Maria and Luke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda836f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mllv = pd.read_excel(path_to_excel+'ml_lv_stats.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.DataFrame(mllv.loc[1,:][1:]).T\n",
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafa132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "\n",
    "X_ml_scaled = scaler_f.transform(ml.values)\n",
    "X_ml_scaled_int = np.insert(X_ml_scaled, 0, 1)\n",
    "X_ml_df = pd.DataFrame(pd.DataFrame(X_ml_scaled_int)).T\n",
    "X_ml_df.columns = pd.Index(['const']).append(X_f.columns)\n",
    "X_ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ml_knn_0_proba = results_f_knn_0.predict(X_ml_df) # add constant to feat\n",
    "y_pred_ml_knn_1_proba = results_f_knn_1.predict(X_ml_df) # add constant to feat\n",
    "y_pred_ml_knn_2_proba = results_f_knn_2.predict(X_ml_df) # add constant to feat\n",
    "y_pred_ml_knn_3_proba = results_f_knn_3.predict(X_ml_df) # add constant to feat\n",
    "y_pred_ml_knn_4_proba = results_f_knn_4.predict(X_ml_df) # add constant to feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_knn_proba = pd.DataFrame({'Leader': y_pred_ml_knn_0_proba,\n",
    "                            'Task orientatied': y_pred_ml_knn_1_proba,\n",
    "                            'Maverick': y_pred_ml_knn_2_proba,\n",
    "                            'Anxious': y_pred_ml_knn_3_proba,\n",
    "                            'Workaholic': y_pred_ml_knn_4_proba})\n",
    "ml_knn_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73477ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = pd.DataFrame(mllv.loc[0,:][1:]).T\n",
    "lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473626aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "\n",
    "X_lv_scaled = scaler_m.transform(lv.values)\n",
    "X_lv_scaled_int = np.insert(X_lv_scaled, 0, 1)\n",
    "X_lv_df = pd.DataFrame(pd.DataFrame(X_lv_scaled_int)).T\n",
    "X_lv_df.columns = pd.Index(['const']).append(X_f.columns)\n",
    "X_lv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lv_knn_0_proba = results_m_knn_0.predict(X_lv_df) # add constant to feat\n",
    "y_pred_lv_knn_1_proba = results_m_knn_1.predict(X_lv_df) # add constant to feat\n",
    "y_pred_lv_knn_2_proba = results_m_knn_2.predict(X_lv_df) # add constant to feat\n",
    "y_pred_lv_knn_3_proba = results_m_knn_3.predict(X_lv_df) # add constant to feat\n",
    "y_pred_lv_knn_4_proba = results_m_knn_4.predict(X_lv_df) # add constant to feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_knn_proba = pd.DataFrame({'Leader': y_pred_lv_knn_0_proba,\n",
    "                            'Task orientatied': y_pred_lv_knn_1_proba,\n",
    "                            'Maverick': y_pred_lv_knn_2_proba,\n",
    "                            'Anxious': y_pred_lv_knn_3_proba,\n",
    "                            'Workaholic': y_pred_lv_knn_4_proba})\n",
    "lv_knn_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef8ffa",
   "metadata": {},
   "source": [
    "### Gaussian clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_gaussian_0 = sm.Logit(y_f_gaussian==0, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_gaussian_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_gaussian_0 = sm.Logit(y_m_gaussian==0, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_gaussian_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_gaussian_1 = sm.Logit(y_f_gaussian==1, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_gaussian_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8081aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_gaussian_1 = sm.Logit(y_m_gaussian==1, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_gaussian_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_gaussian_2 = sm.Logit(y_f_gaussian==2, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_gaussian_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5257763",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_gaussian_2 = sm.Logit(y_m_gaussian==2, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_gaussian_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_gaussian_3 = sm.Logit(y_f_gaussian==3, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_gaussian_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551778a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_gaussian_3 = sm.Logit(y_m_gaussian==3, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_gaussian_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd949625",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f_gaussian_4 = sm.Logit(y_f_gaussian==4, X_f_scaled_int).fit(maxiter=100)\n",
    "results_f_gaussian_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m_gaussian_4 = sm.Logit(y_m_gaussian==4, X_m_scaled_int).fit(maxiter=100)\n",
    "results_m_gaussian_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ac50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y pred proba for the models\n",
    "\n",
    "y_pred_f_gaussian_0_proba = results_f_gaussian_0.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_gaussian_0_proba = results_m_gaussian_0.predict(X_m_scaled_int)\n",
    "y_pred_f_gaussian_1_proba = results_f_gaussian_1.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_gaussian_1_proba = results_m_gaussian_1.predict(X_m_scaled_int)\n",
    "y_pred_f_gaussian_2_proba = results_f_gaussian_2.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_gaussian_2_proba = results_m_gaussian_2.predict(X_m_scaled_int)\n",
    "y_pred_f_gaussian_3_proba = results_f_gaussian_3.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_gaussian_3_proba = results_m_gaussian_3.predict(X_m_scaled_int)\n",
    "y_pred_f_gaussian_4_proba = results_f_gaussian_4.predict(X_f_scaled_int) # add constant to feat\n",
    "y_pred_m_gaussian_4_proba = results_m_gaussian_4.predict(X_m_scaled_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_gaussian_proba = pd.DataFrame({0: y_pred_f_gaussian_0_proba,\n",
    "                                 1: y_pred_f_gaussian_1_proba,\n",
    "                                 2: y_pred_f_gaussian_2_proba,\n",
    "                                 3: y_pred_f_gaussian_3_proba,\n",
    "                                 4: y_pred_f_gaussian_4_proba})\n",
    "f_gaussian_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa37a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_gaussian_proba = pd.DataFrame({0: y_pred_m_gaussian_0_proba,\n",
    "                                 1: y_pred_m_gaussian_1_proba,\n",
    "                                 2: y_pred_m_gaussian_2_proba,\n",
    "                                 3: y_pred_m_gaussian_3_proba,\n",
    "                                 4: y_pred_m_gaussian_4_proba})\n",
    "m_gaussian_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Female models')\n",
    "print('')\n",
    "print(classification_report(y_f_gaussian, np.argmax(f_gaussian_proba.values, axis = 1)))\n",
    "print('')\n",
    "print('Male models')\n",
    "print('')\n",
    "print(classification_report(y_m_gaussian, np.argmax(m_gaussian_proba.values, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b331d",
   "metadata": {},
   "source": [
    "## Feature selection created targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_0_f_best_feats = pd.DataFrame([results_f_knn_0.params, results_f_knn_0.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_0_f_best_feats[KNN_0_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_1_f_best_feats = pd.DataFrame([results_f_knn_1.params, results_f_knn_1.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_1_f_best_feats[KNN_1_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53860f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_2_f_best_feats = pd.DataFrame([results_f_knn_2.params, results_f_knn_2.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_2_f_best_feats[KNN_2_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce783ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_3_f_best_feats = pd.DataFrame([results_f_knn_3.params, results_f_knn_3.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_3_f_best_feats[KNN_3_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_4_f_best_feats = pd.DataFrame([results_f_knn_4.params, results_f_knn_4.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_4_f_best_feats[KNN_1_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_0_m_best_feats = pd.DataFrame([results_m_knn_0.params, results_m_knn_0.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_0_m_best_feats[KNN_0_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_1_m_best_feats = pd.DataFrame([results_m_knn_1.params, results_m_knn_1.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_1_m_best_feats[KNN_1_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_2_m_best_feats = pd.DataFrame([results_m_knn_2.params, results_m_knn_2.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_2_m_best_feats[KNN_2_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_3_m_best_feats = pd.DataFrame([results_m_knn_3.params, results_m_knn_3.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_3_m_best_feats[KNN_3_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_4_m_best_feats = pd.DataFrame([results_m_knn_4.params, results_m_knn_4.pvalues], index=['coef', 'p-value']).T\n",
    "KNN_4_m_best_feats[KNN_4_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09121e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_0_f_best_feats = pd.DataFrame([results_f_gaussian_0.params, results_f_gaussian_0.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_0_f_best_feats[gaussian_0_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_1_f_best_feats = pd.DataFrame([results_f_gaussian_1.params, results_f_gaussian_1.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_1_f_best_feats[gaussian_1_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_2_f_best_feats = pd.DataFrame([results_f_gaussian_2.params, results_f_gaussian_2.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_2_f_best_feats[gaussian_2_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82546d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_3_f_best_feats = pd.DataFrame([results_f_gaussian_3.params, results_f_gaussian_3.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_3_f_best_feats[gaussian_3_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_4_f_best_feats = pd.DataFrame([results_f_gaussian_4.params, results_f_gaussian_4.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_4_f_best_feats[gaussian_4_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf71283",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_0_m_best_feats = pd.DataFrame([results_m_gaussian_0.params, results_m_gaussian_0.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_0_m_best_feats[gaussian_0_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8477a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_1_m_best_feats = pd.DataFrame([results_m_gaussian_1.params, results_m_gaussian_1.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_1_m_best_feats[gaussian_1_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b25482",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_2_m_best_feats = pd.DataFrame([results_m_gaussian_2.params, results_m_gaussian_2.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_2_m_best_feats[gaussian_2_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_3_m_best_feats = pd.DataFrame([results_m_gaussian_3.params, results_m_gaussian_3.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_3_m_best_feats[gaussian_3_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28084c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_4_m_best_feats = pd.DataFrame([results_m_gaussian_4.params, results_m_gaussian_4.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_4_m_best_feats[gaussian_4_m_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6824d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_knn_0.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_knn_0.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_knn_0_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==0, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_knn_0 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_knn_0_score.importances_mean)).T) # Unstack results\n",
    "importance_f_knn_0.columns=['feature','score decrease']\n",
    "importance_f_knn_0.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_knn_0.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf29c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_knn_1.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_knn_1.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_knn_1_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==1, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_knn_1 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_knn_1_score.importances_mean)).T) # Unstack results\n",
    "importance_f_knn_1.columns=['feature','score decrease']\n",
    "importance_f_knn_1.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_knn_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_knn_2.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_knn_2.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_knn_2_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==2, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_knn_2 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_knn_2_score.importances_mean)).T) # Unstack results\n",
    "importance_f_knn_2.columns=['feature','score decrease']\n",
    "importance_f_knn_2.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_knn_2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f404651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_knn_3.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_knn_3.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_knn_3_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==3, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_knn_3 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_knn_3_score.importances_mean)).T) # Unstack results\n",
    "importance_f_knn_3.columns=['feature','score decrease']\n",
    "importance_f_knn_3.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_knn_3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_knn_4.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_knn_4.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_knn_4_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==4, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_knn_4 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_knn_4_score.importances_mean)).T) # Unstack results\n",
    "importance_f_knn_4.columns=['feature','score decrease']\n",
    "importance_f_knn_4.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_knn_4.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2201cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_gaussian_0.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_gaussian_0.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_gaussian_0_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==0, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_gaussian_0 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_gaussian_0_score.importances_mean)).T) # Unstack results\n",
    "importance_f_gaussian_0.columns=['feature','score decrease']\n",
    "importance_f_gaussian_0.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_gaussian_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_gaussian_1.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_gaussian_1.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_gaussian_1_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==1, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_gaussian_1 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_gaussian_1_score.importances_mean)).T) # Unstack results\n",
    "importance_f_gaussian_1.columns=['feature','score decrease']\n",
    "importance_f_gaussian_1.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_gaussian_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_gaussian_2.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_gaussian_2.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_gaussian_2_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==2, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_gaussian_2 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_gaussian_2_score.importances_mean)).T) # Unstack results\n",
    "importance_f_gaussian_2.columns=['feature','score decrease']\n",
    "importance_f_gaussian_2.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_gaussian_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea56754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_gaussian_3.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_gaussian_3.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_gaussian_3_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==3, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_gaussian_3 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_gaussian_3_score.importances_mean)).T) # Unstack results\n",
    "importance_f_gaussian_3.columns=['feature','score decrease']\n",
    "importance_f_gaussian_3.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_gaussian_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the sklearn logreg model using results from them sm model\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.coef_ = np.array([results_f_gaussian_4.params[1:]])\n",
    "sk_lr.intercept_ = np.array([results_f_gaussian_4.params[0]])\n",
    "sk_lr.classes_ = np.array([0, 1])\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to score\n",
    "# Make sure to use X without the constant\n",
    "permutation_f_gaussian_4_score = permutation_importance(sk_lr, X_f_scaled, y_f_knn==4, scoring='accuracy', n_repeats=10) # Perform Permutation\n",
    "importance_f_gaussian_4 = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_f_gaussian_4_score.importances_mean)).T) # Unstack results\n",
    "importance_f_gaussian_4.columns=['feature','score decrease']\n",
    "importance_f_gaussian_4.sort_values(by=\"score decrease\", ascending = False, inplace=True) # Order by importance\n",
    "importance_f_gaussian_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f00111",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic regression with thickness and volume data only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77043c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Thickness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea536d3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lt_feat = list(X_m_scaled_int.columns)[:36]\n",
    "rt_feat = list(X_m_scaled_int.columns)[38:73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ac0e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_f_scaled_int[lt_feat + rt_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b0ba8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_thick_f_gaussian_0 = sm.Logit(y_f_gaussian==0, X_f_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_f_gaussian_1 = sm.Logit(y_f_gaussian==1, X_f_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_f_gaussian_2 = sm.Logit(y_f_gaussian==2, X_f_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_f_gaussian_3 = sm.Logit(y_f_gaussian==3, X_f_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_f_gaussian_4 = sm.Logit(y_f_gaussian==4, X_f_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873bc56",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_thick_m_gaussian_0 = sm.Logit(y_m_gaussian==0, X_m_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_m_gaussian_1 = sm.Logit(y_m_gaussian==1, X_m_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_m_gaussian_2 = sm.Logit(y_m_gaussian==2, X_m_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_m_gaussian_3 = sm.Logit(y_m_gaussian==3, X_m_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)\n",
    "results_thick_m_gaussian_4 = sm.Logit(y_m_gaussian==4, X_m_scaled_int[lt_feat + rt_feat]).fit(maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bcba4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make y pred proba for the models\n",
    "\n",
    "y_pred_thick_f_gaussian_0_proba = results_thick_f_gaussian_0.predict(X_f_scaled_int[lt_feat + rt_feat]) # add constant to feat\n",
    "y_pred_thick_m_gaussian_0_proba = results_thick_m_gaussian_0.predict(X_m_scaled_int[lt_feat + rt_feat])\n",
    "y_pred_thick_f_gaussian_1_proba = results_thick_f_gaussian_1.predict(X_f_scaled_int[lt_feat + rt_feat]) # add constant to feat\n",
    "y_pred_thick_m_gaussian_1_proba = results_thick_m_gaussian_1.predict(X_m_scaled_int[lt_feat + rt_feat])\n",
    "y_pred_thick_f_gaussian_2_proba = results_thick_f_gaussian_2.predict(X_f_scaled_int[lt_feat + rt_feat]) # add constant to feat\n",
    "y_pred_thick_m_gaussian_2_proba = results_thick_m_gaussian_2.predict(X_m_scaled_int[lt_feat + rt_feat])\n",
    "y_pred_thick_f_gaussian_3_proba = results_thick_f_gaussian_3.predict(X_f_scaled_int[lt_feat + rt_feat]) # add constant to feat\n",
    "y_pred_thick_m_gaussian_3_proba = results_thick_m_gaussian_3.predict(X_m_scaled_int[lt_feat + rt_feat])\n",
    "y_pred_thick_f_gaussian_4_proba = results_thick_f_gaussian_4.predict(X_f_scaled_int[lt_feat + rt_feat]) # add constant to feat\n",
    "y_pred_thick_m_gaussian_4_proba = results_thick_m_gaussian_4.predict(X_m_scaled_int[lt_feat + rt_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbab2c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_thick_gaussian_proba = pd.DataFrame({0: y_pred_thick_f_gaussian_0_proba,\n",
    "                                       1: y_pred_thick_f_gaussian_1_proba,\n",
    "                                       2: y_pred_thick_f_gaussian_2_proba,\n",
    "                                       3: y_pred_thick_f_gaussian_3_proba,\n",
    "                                       4: y_pred_thick_f_gaussian_4_proba})\n",
    "\n",
    "m_thick_gaussian_proba = pd.DataFrame({0: y_pred_thick_m_gaussian_0_proba,\n",
    "                                       1: y_pred_thick_m_gaussian_1_proba,\n",
    "                                       2: y_pred_thick_m_gaussian_2_proba,\n",
    "                                       3: y_pred_thick_m_gaussian_3_proba,\n",
    "                                       4: y_pred_thick_m_gaussian_4_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c812c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_f_gaussian, np.argmax(f_thick_gaussian_proba.values, axis = 1)))\n",
    "print(classification_report(y_m_gaussian, np.argmax(m_thick_gaussian_proba.values, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b80368",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Volume data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff28049",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vol_feat_1 = list(X_f_scaled_int.columns)[0]\n",
    "vol_feat_2 = list(X_f_scaled_int.columns)[36:38]\n",
    "vol_feat_3 = list(X_f_scaled_int.columns)[73:]\n",
    "X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a17a26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_vol_f_gaussian_0 = sm.Logit(y_f_gaussian==0, X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_f_gaussian_1 = sm.Logit(y_f_gaussian==1, X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_f_gaussian_2 = sm.Logit(y_f_gaussian==2, X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_f_gaussian_3 = sm.Logit(y_f_gaussian==3, X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_f_gaussian_4 = sm.Logit(y_f_gaussian==4, X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd38f19",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_vol_m_gaussian_0 = sm.Logit(y_m_gaussian==0, X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_m_gaussian_1 = sm.Logit(y_m_gaussian==1, X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_m_gaussian_2 = sm.Logit(y_m_gaussian==2, X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_m_gaussian_3 = sm.Logit(y_m_gaussian==3, X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)\n",
    "results_vol_m_gaussian_4 = sm.Logit(y_m_gaussian==4, X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]).fit(maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31dec8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_vol_f_gaussian_0_proba = results_vol_f_gaussian_0.predict(X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]) # add constant to feat\n",
    "y_pred_vol_m_gaussian_0_proba = results_vol_m_gaussian_0.predict(X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3])\n",
    "y_pred_vol_f_gaussian_1_proba = results_vol_f_gaussian_1.predict(X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]) # add constant to feat\n",
    "y_pred_vol_m_gaussian_1_proba = results_vol_m_gaussian_1.predict(X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3])\n",
    "y_pred_vol_f_gaussian_2_proba = results_vol_f_gaussian_2.predict(X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]) # add constant to feat\n",
    "y_pred_vol_m_gaussian_2_proba = results_vol_m_gaussian_2.predict(X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3])\n",
    "y_pred_vol_f_gaussian_3_proba = results_vol_f_gaussian_3.predict(X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]) # add constant to feat\n",
    "y_pred_vol_m_gaussian_3_proba = results_vol_m_gaussian_3.predict(X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3])\n",
    "y_pred_vol_f_gaussian_4_proba = results_vol_f_gaussian_4.predict(X_f_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3]) # add constant to feat\n",
    "y_pred_vol_m_gaussian_4_proba = results_vol_m_gaussian_4.predict(X_m_scaled_int[[vol_feat_1] + vol_feat_2 + vol_feat_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3991f89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_vol_gaussian_proba = pd.DataFrame({0: y_pred_vol_f_gaussian_0_proba,\n",
    "                                       1: y_pred_vol_f_gaussian_0_proba,\n",
    "                                       2: y_pred_vol_f_gaussian_0_proba,\n",
    "                                       3: y_pred_vol_f_gaussian_0_proba,\n",
    "                                       4: y_pred_vol_f_gaussian_0_proba})\n",
    "\n",
    "m_vol_gaussian_proba = pd.DataFrame({0: y_pred_vol_m_gaussian_0_proba,\n",
    "                                       1: y_pred_vol_m_gaussian_0_proba,\n",
    "                                       2: y_pred_vol_m_gaussian_0_proba,\n",
    "                                       3: y_pred_vol_m_gaussian_0_proba,\n",
    "                                       4: y_pred_vol_m_gaussian_0_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932224af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_f_gaussian, np.argmax(f_vol_gaussian_proba.values, axis = 1)))\n",
    "print(classification_report(y_m_gaussian, np.argmax(m_vol_gaussian_proba.values, axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbd5b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gaussian_0_thick_f_best_feats = pd.DataFrame([results_thick_f_gaussian_0.params, results_thick_f_gaussian_0.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_0_thick_f_best_feats[gaussian_0_thick_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce25373",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gaussian_1_thick_f_best_feats = pd.DataFrame([results_thick_f_gaussian_1.params, results_thick_f_gaussian_1.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_1_thick_f_best_feats[gaussian_1_thick_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e484f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gaussian_2_thick_f_best_feats = pd.DataFrame([results_thick_f_gaussian_2.params, results_thick_f_gaussian_2.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_2_thick_f_best_feats[gaussian_2_thick_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e600a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gaussian_3_thick_f_best_feats = pd.DataFrame([results_thick_f_gaussian_3.params, results_thick_f_gaussian_3.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_3_thick_f_best_feats[gaussian_3_thick_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed2bbb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gaussian_4_thick_f_best_feats = pd.DataFrame([results_thick_f_gaussian_4.params, results_thick_f_gaussian_4.pvalues], index=['coef', 'p-value']).T\n",
    "gaussian_4_thick_f_best_feats[gaussian_4_thick_f_best_feats['p-value']<0.05].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d25d5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing quartile targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05639a99",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt=QuantileTransformer(n_quantiles=1350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a8e16",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a37a45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qt.fit_transform(y_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4d1dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantile_df=pd.DataFrame(qt.fit_transform(y_disc),columns=y_disc.columns)\n",
    "quantile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340122e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quant_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "quantile_df=pd.DataFrame(quant_imputer.fit_transform(quantile_df),columns=quantile_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426f1b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantile_kmeans=KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7094722",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantile_kmeans.fit_predict(quantile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5448ab4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantile_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fe792",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantile_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518d073",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quant_target=quantile_kmeans.labels_\n",
    "df['quant_target']=quant_target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3185274",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_quant=df['quant_target']\n",
    "y_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457924d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe46975",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sex_df2 = df\n",
    "sex_df2.sex.replace('female', 'F', inplace=True)\n",
    "sex_df2.sex.replace('male', 'M', inplace=True)\n",
    "sex_df2.sex.replace(np.nan, 'F', inplace=True)\n",
    "f_df2 = sex_df2[sex_df2.sex == 'F']\n",
    "m_df2 = sex_df2[sex_df2.sex == 'M']\n",
    "sex_df2.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49d850",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Feature creation\n",
    "X_f2 = f_df2.iloc[:,11:].reset_index(drop=True).drop(columns='quant_target')\n",
    "X_m2 = m_df2.iloc[:,11:].reset_index(drop=True).drop(columns='quant_target')\n",
    "y_f2 = f_df2.quant_target.reset_index(drop=True)\n",
    "y_m2 = m_df2.quant_target.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f650ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add a constant as a feature to the scaled X\n",
    "X_f_2_int = sm.add_constant(X_f2)\n",
    "X_f_2_int = pd.DataFrame(X_f_2_int, columns=pd.Index(['const']).append(X_f2.columns))\n",
    "\n",
    "X_m_2_int = sm.add_constant(X_m2)\n",
    "X_m_2_int = pd.DataFrame(X_m_2_int, columns=pd.Index(['const']).append(X_f2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783f84f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_f_q_0 = sm.Logit(y_f2==0, X_f_2_int).fit(maxiter=1000)\n",
    "results_f_q_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae2033",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# results_f_q_1 = sm.Logit(y_f2==1, X_f_2_int).fit(maxiter=1000)\n",
    "# results_f_q_2 = sm.Logit(y_f2==2, X_f_2_int).fit(maxiter=1000)\n",
    "# results_f_q_3 = sm.Logit(y_f2==3, X_f_2_int).fit(maxiter=1000)\n",
    "# results_f_q_4 = sm.Logit(y_f2==4, X_f_2_int).fit(maxiter=1000)\n",
    "\n",
    "# results_m_q_0 = sm.Logit(y_m2==0, X_m_2_int).fit(maxiter=1000)\n",
    "# results_m_q_1 = sm.Logit(y_m2==1, X_m_2_int).fit(maxiter=1000)\n",
    "# results_m_q_2 = sm.Logit(y_m2==2, X_m_2_int).fit(maxiter=1000)\n",
    "# results_m_q_3 = sm.Logit(y_m2==3, X_m_2_int).fit(maxiter=1000)\n",
    "# results_m_q_4 = sm.Logit(y_m2==4, X_m_2_int).fit(maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da3c7a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_pred_f_q_0_proba = results_f_q_0.predict(X_f_2_int) # add constant to feat\n",
    "# y_pred_f_q_1_proba = results_f_q_1.predict(X_f_2_int)\n",
    "# y_pred_f_q_2_proba = results_f_q_2.predict(X_f_2_int) # add constant to feat\n",
    "# y_pred_f_q_3_proba = results_f_q_3.predict(X_f_2_int)\n",
    "# y_pred_f_q_4_proba = results_f_q_4.predict(X_f_2_int) # add constant to feat\n",
    "# y_pred_m_q_0_proba = results_f_q_0.predict(X_m_2_int)\n",
    "# y_pred_m_q_1_proba = results_f_q_1.predict(X_m_2_int) # add constant to feat\n",
    "# y_pred_m_q_2_proba = results_f_q_2.predict(X_m_2_int)\n",
    "# y_pred_m_q_3_proba = results_f_q_3.predict(X_m_2_int) # add constant to feat\n",
    "# y_pred_m_q_4_proba = results_f_q_4.predict(X_m_2_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d6852",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# f_g_proba = pd.DataFrame({0: y_pred_f_q_0_proba,\n",
    "#                                  1: y_pred_f_q_1_proba,\n",
    "#                                  2: y_pred_f_q_2_proba,\n",
    "#                                  3: y_pred_f_q_3_proba,\n",
    "#                                  4: y_pred_f_q_4_proba})\n",
    "# f_g_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c8796",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# m_g_proba = pd.DataFrame({0: y_pred_m_q_0_proba,\n",
    "#                                  1: y_pred_m_q_1_proba,\n",
    "#                                  2: y_pred_m_q_2_proba,\n",
    "#                                  3: y_pred_m_q_3_proba,\n",
    "#                                  4: y_pred_m_q_4_proba})\n",
    "# m_g_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19899255",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(classification_report(y_f2, np.argmax(f_g_proba.values, axis = 1)))\n",
    "# print(classification_report(y_m2, np.argmax(m_g_proba.values, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773de66",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do permutation and coef with sex. Cars exercise.\n",
    "# permutation is taking out one col at a time and working out the drop in score of the sklearn model- R2 in regression, acc in classification\n",
    "# VIF- only good for linear models\n",
    "# Work out different models for sex\n",
    "# Work out for different characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7762c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
